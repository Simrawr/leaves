{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simrawr/leaves/blob/main/fake_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yapsxI_Wo0Nv",
        "outputId": "77aa59a8-c868-4500-df0b-4d68ef42cd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyIvTBFApsb2",
        "outputId": "395ba482-2427-46d8-b965-e2dcecdd9ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lA0QqvHrcmmy"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms, models\n",
        "data_dir = \"ai_data/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-Tstp0jyA63",
        "outputId": "09e43ae0-973f-4c52-80ec-7f321a5a7d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqZDVMyVKDnD",
        "outputId": "86bc07e8-4f2f-4cba-817e-ed5c253af50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8_WbAILKng-"
      },
      "outputs": [],
      "source": [
        "!unzip gdrive/My\\ Drive/ai_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpJIDl8FL5Bo"
      },
      "source": [
        "alt: !unzip data2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "t6j4eavsnwCw"
      },
      "outputs": [],
      "source": [
        "train_data = datasets.ImageFolder(data_dir + '/train')\n",
        "test_data = datasets.ImageFolder(data_dir + '/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KrH9G81n0Qq",
        "outputId": "24bd2335-d676-421e-81b1-6f2ca26e38e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([8, 3, 224, 224])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "train_transforms = transforms.Compose([transforms.RandomRotation(30), #data augumnetation\n",
        "                                       transforms.RandomResizedCrop(224),#resize\n",
        "                                       transforms.RandomHorizontalFlip(), #data augumnetation\n",
        "                                       transforms.ToTensor()])\n",
        "test_transforms = transforms.Compose([transforms.RandomResizedCrop(224), #resize\n",
        "                                      transforms.ToTensor()])\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=8)\n",
        "dataiter = iter(train_loader)\n",
        "images, classes  = next(dataiter)\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(classes.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q3mCqRcsnBN",
        "outputId": "b1845bbd-0fa7-45a8-dbb9-fc67787196d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fam8HQk5sgCp",
        "outputId": "f99af944-b779-407e-b354-4ad8e0eaf746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "class CropDetectCNN(nn.Module):\n",
        "    # initialize the class and the parameters\n",
        "    def __init__(self):\n",
        "        super(CropDetectCNN, self).__init__()\n",
        "\n",
        "        # convolutional layer 1 & max pool layer 1\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3),\n",
        "            nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "        # convolutional layer 2 & max pool layer 2\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1, stride=2),\n",
        "            nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "        # convolutional layer 3 & max pool layer 3\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=3),\n",
        "            nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "        # convolutional layer 4 & max pool layer 4\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=2, stride=2),\n",
        "            nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "        #Fully connected layer\n",
        "        self.fc = nn.Linear(128*2*2, 2)\n",
        "\n",
        "    # Feed forward the network\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x.to(device))\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "# model = CropDetectCNN().to(device)\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h10NTjP0b8b",
        "outputId": "b34a968f-3b37-4613-d21c-f976a98c107b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print (device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7JxGCAmfsmoi"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6MavrS6IsrBo"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tud74b5XqHNT",
        "outputId": "395c8c9c-208a-4f10-eb96-153a099c020b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZwZfoSldIst"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJKMj1cHcDNF",
        "outputId": "6175f5eb-8dc1-41a4-e449-e776300a4686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ImageFolder\n",
            "    Number of datapoints: 100000\n",
            "    Root location: ai_data//train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               RandomRotation(degrees=[-30.0, 30.0], interpolation=nearest, expand=False, fill=0)\n",
            "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=warn)\n",
            "               RandomHorizontalFlip(p=0.5)\n",
            "               ToTensor()\n",
            "           )\n",
            "['FAKE', 'REAL']\n"
          ]
        }
      ],
      "source": [
        "print(train_data)\n",
        "print(train_data.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMw6mYX2tFdH",
        "outputId": "f485d9ff-5aeb-46fa-83b0-2d23586dec74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/1..  Training Loss: 0.356..  Valid Loss: 0.407..  Valid Accuracy: 0.816\n"
          ]
        }
      ],
      "source": [
        "epochs = 1 # run more iterations\n",
        "# if torch.cuda.is_available():\n",
        "  # model.cuda()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, classes in train_loader:\n",
        "        # To device - to transfrom the image and classes to CPU|GPU\n",
        "        images, classes = images.to(device), classes.to(device)\n",
        "\n",
        "        # clears old gradients from the last step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # train the images\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "\n",
        "        #calculate the loss given the outputs and the classes\n",
        "        loss = criterion(outputs, classes)\n",
        "\n",
        "        # compute the loss of every parameter\n",
        "        loss.backward()\n",
        "\n",
        "        # apply the optimizer and its parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        #update the loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    else:\n",
        "        validation_loss = 0\n",
        "        accuracy = 0\n",
        "\n",
        "        # to make the model run faster we are using the gradients on the train\n",
        "        with torch.no_grad():\n",
        "            # specify that this is validation and not training\n",
        "            model.eval()\n",
        "            for images, classes in test_loader:\n",
        "                # Use GPU\n",
        "                images, classes = images.to(device), classes.to(device)\n",
        "\n",
        "                # validate the images\n",
        "                outputs = model(images)\n",
        "\n",
        "                # compute validation loss\n",
        "                loss = criterion(outputs, classes)\n",
        "\n",
        "                #update loss\n",
        "                validation_loss += loss.item()\n",
        "\n",
        "                # get the exponential of the outputs\n",
        "                ps = torch.exp(outputs)\n",
        "\n",
        "                #Returns the k largest elements of the given input tensor along a given dimension.\n",
        "                top_p, top_class = ps.topk(1, dim=1)\n",
        "\n",
        "                # reshape the tensor\n",
        "                equals = top_class == classes.view(*top_class.shape)\n",
        "\n",
        "                # calculate the accuracy.\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "        # change the mode to train for the next epochs\n",
        "        model.train()\n",
        "        print(\"Epoch: {}/{}.. \".format(epoch+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
        "              \"Valid Loss: {:.3f}.. \".format(validation_loss/len(test_loader)),\n",
        "              \"Valid Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqJfs93gqIHe",
        "outputId": "75042c22-5d7c-469d-bdff-f0d9b77c4c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "923hjuPnOBY7",
        "outputId": "b2182fb0-a2dd-45bb-9179-1fa7ea7f382f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('FAKE', 0), ('REAL', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "model.class_to_idx = train_data.class_to_idx\n",
        "model.class_to_idx.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "vjT79M3Hj0qW"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "# Plot the image\n",
        "def imshow(image_numpy_array):\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # convert the shape from (3, 256, 256) to (256, 256, 3)\n",
        "    image = image_numpy_array.transpose(0, 1, 2)\n",
        "    ax.imshow(image)\n",
        "    ax.set_xticklabels('')\n",
        "    ax.set_yticklabels('')\n",
        "\n",
        "    return ax\n",
        "def process_image(image_path):\n",
        "\n",
        "    test_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                         transforms.ToTensor()])\n",
        "\n",
        "    im = Image.open(image_path)\n",
        "    imshow(np.array(im))\n",
        "    im = test_transform(im)\n",
        "    return im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "eIoD4f7Xj5Ne"
      },
      "outputs": [],
      "source": [
        "def predict(image, model):\n",
        "    # we have to process the image as we did while training the others\n",
        "    image = process_image(image)\n",
        "\n",
        "    #returns a new tensor with a given dimension\n",
        "    image_input = image.unsqueeze(0)\n",
        "\n",
        "    # Convert the image to either gpu|cpu\n",
        "    image_input.to(device)\n",
        "\n",
        "    # Pass the image through the model\n",
        "    outputs = model(image_input).to(device)\n",
        "    ps = torch.exp(outputs)\n",
        "\n",
        "    # return the top 5 most predicted classes\n",
        "    top_p, top_cls = ps.topk(5, dim=1)\n",
        "    # convert to numpy, then to list\n",
        "    top_cls = top_cls.detach().cpu().numpy().tolist()[0]\n",
        "\n",
        "    # covert indices to classes\n",
        "    idx_to_class = {v: k for k, v in model.class_to_idx.items()}\n",
        "\n",
        "    top_cls = [idx_to_class[top_class] for top_class in top_cls]\n",
        "\n",
        "    return top_p, top_cls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DmPQUbRpj8YF"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_solution(image_path, ps, classes):\n",
        "    plt.figure(figsize = (6,10))\n",
        "\n",
        "    image = process_image(image_path)\n",
        "    plt.subplot(2,1,2)\n",
        "    sns.barplot(x=ps, y=classes, color=sns.color_palette()[2]);\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "qFc5KpCEkDFg",
        "outputId": "c40aa9e4-1f1c-493b-ce13-4dc1cbfa3081"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-168c46f4b665>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ai_data/train/FAKE/1000 (8).jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-dfadd4ac6176>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Pass the image through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/alexnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "image = \"ai_data/train/FAKE/1000 (8).jpg\"\n",
        "ps, classes = predict(image, model)\n",
        "ps = ps.detach().cpu().numpy().tolist()[0]\n",
        "print(ps)\n",
        "print(classes)\n",
        "plot_solution(image, ps, classes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1xV_adVeUswGbyJisNS9oSLNJ605ZMpLh",
      "authorship_tag": "ABX9TyNw4cw0VDoKHxMNstjPb+z3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}